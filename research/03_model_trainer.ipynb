{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea1bdf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccf59fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f:\\\\end-to-end-deep_learning-project\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e68e0cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f:\\\\end-to-end-deep_learning-project'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efdb81c",
   "metadata": {},
   "source": [
    "# In the model_trainer_stage, your goal is simple:\n",
    "\n",
    "â­ Take the updated base model â†’ Train it â†’ Save the trained model.\n",
    "\n",
    "No confusion.\n",
    "No extras.\n",
    "Here are the only tasks the training stage must do.\n",
    "\n",
    "âœ… 1. Load the Updated Base Model\n",
    "\n",
    "You previously saved:\n",
    "\n",
    "base_model.pt â†’ frozen base\n",
    "\n",
    "base_model_updated.pt â†’ base + new classifier\n",
    "\n",
    "Training stage ALWAYS loads:\n",
    "\n",
    "updated_base_model_path     (e.g. base_model_updated.pth)\n",
    "\n",
    "âœ… 2. Load Training Data\n",
    "\n",
    "Your config already gives you:\n",
    "\n",
    "training_data = Path(training_data_dir)\n",
    "\n",
    "\n",
    "Model trainer must:\n",
    "\n",
    "Use ImageFolder\n",
    "\n",
    "Apply transforms\n",
    "\n",
    "Create DataLoader\n",
    "\n",
    "âœ… 3. Set Up Loss & Optimizer\n",
    "\n",
    "Because you already\n",
    "\n",
    "froze feature layers\n",
    "\n",
    "unfroze classifier only\n",
    "\n",
    "You must define:\n",
    "\n",
    "CrossEntropyLoss\n",
    "\n",
    "Adam(model.classifier.parameters(), lr=...)\n",
    "\n",
    "âœ… 4. Run the Training Loop\n",
    "\n",
    "For each epoch:\n",
    "\n",
    "iterate through dataloader\n",
    "\n",
    "forward pass\n",
    "\n",
    "compute loss\n",
    "\n",
    "backward pass\n",
    "\n",
    "optimizer step\n",
    "\n",
    "accumulate loss\n",
    "\n",
    "âœ… 5. Save the Trained Model\n",
    "\n",
    "Save as:\n",
    "\n",
    "trained_model_path (e.g. artifacts/training/model.pth)\n",
    "\n",
    "\n",
    "This is your final trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacc0a8b",
   "metadata": {},
   "source": [
    "# ðŸŒŸ What your trainer stage code will logically look like\n",
    "\n",
    "(no code, just logic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6461886",
   "metadata": {},
   "source": [
    "Load updated_base_model\n",
    "\n",
    "Load training dataset from folder\n",
    "\n",
    "Apply transforms (resize, normalize)\n",
    "\n",
    "Create DataLoader\n",
    "\n",
    "Move model to device\n",
    "\n",
    "Define loss + optimizer\n",
    "\n",
    "For each epoch:\n",
    "\n",
    "    For each batch:\n",
    "\n",
    "        Forward pass\n",
    "\n",
    "        Compute loss\n",
    "\n",
    "        Backward pass\n",
    "        \n",
    "        Update parameters\n",
    "\n",
    "    Print epoch loss\n",
    "    \n",
    "Save trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b6c2d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    \n",
    "    root_dir: Path\n",
    "    trained_model_path: Path\n",
    "    updated_base_model_path: Path\n",
    "    training_data: Path\n",
    "    params_epochs: int\n",
    "    params_batch_size: int\n",
    "    params_is_augmentation: bool\n",
    "    params_image_size: list\n",
    "    params_learning_rate:float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1378618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnnClassifier.constants import *\n",
    "from cnnClassifier.utils.common import read_yaml, create_directories\n",
    "#from cnnClassifier.utils.common import read_yaml, create_directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "150e9ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "        \n",
    "\n",
    "    def get_training_config(self) -> TrainingConfig:\n",
    "        training = self.config.training\n",
    "        prepare_base_model = self.config.prepare_base_model\n",
    "        params = self.params\n",
    "        #training_data = os.path.join(self.config.data_ingestion.unzip_dir, \"Chest-CT-Scan-data\")\n",
    "        training_data_path = self.config.data_ingestion.unzip_dir\n",
    "        \n",
    "        create_directories([\n",
    "            Path(training.root_dir)\n",
    "        ])\n",
    "\n",
    "        training_config = TrainingConfig(\n",
    "            root_dir=Path(training.root_dir),\n",
    "            trained_model_path=Path(training.trained_model_path),\n",
    "            updated_base_model_path=Path(prepare_base_model.updated_base_model_path),\n",
    "            training_data=Path(training_data_path),\n",
    "            params_epochs=params.EPOCHS,\n",
    "            params_batch_size=params.BATCH_SIZE,\n",
    "            params_is_augmentation=params.AUGMENTATION,\n",
    "            params_image_size=params.IMAGE_SIZE,\n",
    "            params_learning_rate=params.LEARNING_RATE\n",
    "        )\n",
    "\n",
    "        return training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b5025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from torchvision.datasets import ImageFolder # class \n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split # function\n",
    "from torchvision import transforms # module\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c4a86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training:\n",
    "\n",
    "  def __init__(self,config:TrainingConfig):\n",
    "    \n",
    "    self.config = config\n",
    "    self.model = torch.load(self.config.updated_base_model_path)\n",
    "    self.train_loader = None\n",
    "    self.val_loader = None\n",
    "    self.loss_func = None\n",
    "    self.opt = None\n",
    "    self.train_data = None\n",
    "    self.val_data = None\n",
    "    self.train_loader = None\n",
    "    self.val_loader = None\n",
    "\n",
    "\n",
    "  def load_data(self):\n",
    "\n",
    "    data_path = Path(self.config.training_data)\n",
    "    img_size = self.config.params_image_size\n",
    "\n",
    "    transform = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                    transforms.ToTensor()\n",
    "                                  ])\n",
    "\n",
    "    dataset = ImageFolder(data_path,transform=transform)\n",
    "    self.split_dataset(dataset)\n",
    "    \n",
    "\n",
    "  \n",
    "  def split_dataset(self,dataset):\n",
    "\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "\n",
    "    self.train_data, self.val_data = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    \n",
    "  def setup_training(self):\n",
    "\n",
    "    \n",
    "    batch_size = self.config.params_batch_size\n",
    "    lr = self.config.params_learning_rate\n",
    "\n",
    "    self.train_loader = DataLoader(self.train_data, batch_size=batch_size, shuffle=True)\n",
    "    self.val_loader = DataLoader(self.val_data, batch_size=batch_size)\n",
    "    self.loss_func = nn.CrossEntropyLoss()\n",
    "    self.optimizer = torch.optim.Adam(self.model.classifier.parameters(), lr=lr)\n",
    "\n",
    "  @staticmethod\n",
    "  def accuracy(out,label):\n",
    "\n",
    "    preds = torch.argmax(out,dim=1)\n",
    "    batch_acc= torch.sum(preds == label) / len(label)\n",
    "    return batch_acc\n",
    "\n",
    "\n",
    "  def train_model(self, loader=None, opt=None):\n",
    "\n",
    "    losses = []\n",
    "    accs = []\n",
    "    total_samples = 0\n",
    "\n",
    "    # -------------------------\n",
    "    # TRAINING mode\n",
    "    # -------------------------\n",
    "    if opt is not None:\n",
    "        self.model.train()\n",
    "\n",
    "        for xb, yb in loader:\n",
    "            out = self.model(xb)\n",
    "            batch_loss = self.loss_func(out, yb)\n",
    "            batch_acc = self.accuracy(out, yb)\n",
    "\n",
    "            # update classifier parameters\n",
    "            opt.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            batch_size = len(xb)\n",
    "            losses.append(batch_loss.item() * batch_size)\n",
    "            accs.append(batch_acc.item() * batch_size)\n",
    "            total_samples += batch_size\n",
    "\n",
    "\n",
    "    # -------------------------\n",
    "    # VALIDATION mode\n",
    "    # -------------------------\n",
    "    else:\n",
    "        self.model.eval()\n",
    "\n",
    "        with torch.no_grad():       # <---- This is all you need\n",
    "            for xb, yb in loader:\n",
    "                out = self.model(xb)\n",
    "                batch_loss = self.loss_func(out, yb)\n",
    "                batch_acc = self.accuracy(out, yb)\n",
    "\n",
    "                batch_size = len(xb)\n",
    "                losses.append(batch_loss.item() * batch_size)\n",
    "                accs.append(batch_acc.item() * batch_size)\n",
    "                total_samples += batch_size\n",
    "\n",
    "\n",
    "    avg_loss = sum(losses) / total_samples\n",
    "    avg_acc = sum(accs) / total_samples\n",
    "\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "\n",
    "  def fit(self):\n",
    "\n",
    "    epochs = self.config.params_epochs\n",
    "    train_losses, train_accs, val_losses, val_accs = [],[],[],[]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      loss, acc = self.train_model(loader=self.train_loader,opt=self.optimizer)\n",
    "      train_losses.append(loss)\n",
    "      train_accs.append(acc)\n",
    "\n",
    "      val_loss, val_acc = self.train_model(loader=self.val_loader,opt=None)\n",
    "      val_losses.append(val_loss)\n",
    "      val_accs.append(val_acc)\n",
    "\n",
    "\n",
    "      print(f\"Epoch {epoch+1} Loss: {loss:.4f} Accuracy : {acc} val_Loss: {val_loss:.4f} val_Accuracy : {val_acc}\")\n",
    "\n",
    "    #return train_losses, train_accs, val_losses, val_accs\n",
    "\n",
    "  \n",
    "  def save_model(self):\n",
    "    \"\"\"Save full PyTorch model\"\"\"\n",
    "    path = self.config.trained_model_path\n",
    "    torch.save(self.model, path)\n",
    "\n",
    "  def run(self):\n",
    "    self.load_data()\n",
    "    self.setup_training()\n",
    "    self.fit()\n",
    "    self.save_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7fa90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-12 15:17:16,409: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-12-12 15:17:16,489: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-12-12 15:17:16,489: INFO: common: created directory at: artifacts]\n",
      "[2025-12-12 15:17:16,500: INFO: common: created directory at: artifacts\\training]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.2817 Accuracy : 0.8584269650866476 val_Loss: 0.1732 val_Accuracy : 0.9017857142857143\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    training_config = config.get_training_config()\n",
    "    training = Training(config=training_config)\n",
    "    training.run()\n",
    "    \n",
    "except Exception as e:\n",
    "    raise e "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bc94e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
